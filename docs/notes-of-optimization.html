<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Notes of Optimization | Notes for Social Network Science Projects using R</title>
  <meta name="description" content="2 Notes of Optimization | Notes for Social Network Science Projects using R" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Notes of Optimization | Notes for Social Network Science Projects using R" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Notes of Optimization | Notes for Social Network Science Projects using R" />
  
  
  



<meta name="date" content="2020-09-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="notes-of-r-and-python.html"/>
<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Notes for Social Network Science Projects using R</a></li>
<li class="chapter" data-level="2" data-path="notes-of-optimization.html"><a href="notes-of-optimization.html"><i class="fa fa-check"></i><b>2</b> Notes of Optimization</a>
<ul>
<li class="chapter" data-level="2.1" data-path="notes-of-optimization.html"><a href="notes-of-optimization.html#notes-of-linear-regression"><i class="fa fa-check"></i><b>2.1</b> Notes of Linear Regression</a></li>
<li class="chapter" data-level="2.2" data-path="notes-of-optimization.html"><a href="notes-of-optimization.html#logarithms"><i class="fa fa-check"></i><b>2.2</b> Logarithms</a></li>
<li class="chapter" data-level="2.3" data-path="notes-of-optimization.html"><a href="notes-of-optimization.html#linear-regression"><i class="fa fa-check"></i><b>2.3</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="notes-of-optimization.html"><a href="notes-of-optimization.html#gaussian-elimination"><i class="fa fa-check"></i><b>2.3.1</b> Gaussian elimination</a></li>
<li class="chapter" data-level="2.3.2" data-path="notes-of-optimization.html"><a href="notes-of-optimization.html#qr-decomposition"><i class="fa fa-check"></i><b>2.3.2</b> QR decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="notes-of-optimization.html"><a href="notes-of-optimization.html#multivariate-normal-distribution"><i class="fa fa-check"></i><b>2.4</b> Multivariate Normal Distribution</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="notes-of-optimization.html"><a href="notes-of-optimization.html#choleskey-decomposition"><i class="fa fa-check"></i><b>2.4.1</b> Choleskey decomposition</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="notes-of-r-and-python.html"><a href="notes-of-r-and-python.html"><i class="fa fa-check"></i><b>3</b> Notes of R and Python</a>
<ul>
<li class="chapter" data-level="3.1" data-path="notes-of-r-and-python.html"><a href="notes-of-r-and-python.html#connecting-r-and-python-for-social-network-science-projects"><i class="fa fa-check"></i><b>3.1</b> Connecting R and Python for Social Network Science Projects</a></li>
<li class="chapter" data-level="3.2" data-path="notes-of-r-and-python.html"><a href="notes-of-r-and-python.html#conda-environment"><i class="fa fa-check"></i><b>3.2</b> Conda environment</a></li>
<li class="chapter" data-level="3.3" data-path="notes-of-r-and-python.html"><a href="notes-of-r-and-python.html#installing-social-network-science-modules-from-python"><i class="fa fa-check"></i><b>3.3</b> Installing Social Network Science modules from Python</a></li>
<li class="chapter" data-level="3.4" data-path="notes-of-r-and-python.html"><a href="notes-of-r-and-python.html#connecting-r-and-python"><i class="fa fa-check"></i><b>3.4</b> Connecting R and Python</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="notes-of-c-and-r.html"><a href="notes-of-c-and-r.html"><i class="fa fa-check"></i><b>4</b> Notes of C++ and R</a>
<ul>
<li class="chapter" data-level="4.1" data-path="notes-of-c-and-r.html"><a href="notes-of-c-and-r.html#c-and-rcpp"><i class="fa fa-check"></i><b>4.1</b> C++ and Rcpp</a></li>
<li class="chapter" data-level="4.2" data-path="notes-of-c-and-r.html"><a href="notes-of-c-and-r.html#rcpp-and-social-network-analysis"><i class="fa fa-check"></i><b>4.2</b> Rcpp and Social Network Analysis</a></li>
<li class="chapter" data-level="4.3" data-path="notes-of-c-and-r.html"><a href="notes-of-c-and-r.html#setup"><i class="fa fa-check"></i><b>4.3</b> Setup</a></li>
<li class="chapter" data-level="4.4" data-path="notes-of-c-and-r.html"><a href="notes-of-c-and-r.html#rcpp-introduction"><i class="fa fa-check"></i><b>4.4</b> Rcpp Introduction</a></li>
<li class="chapter" data-level="4.5" data-path="notes-of-c-and-r.html"><a href="notes-of-c-and-r.html#d.-eddelbuettel-tutorial"><i class="fa fa-check"></i><b>4.5</b> D. Eddelbuettel tutorial</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes for Social Network Science Projects using R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="notes-of-optimization" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Notes of Optimization</h1>
<div id="notes-of-linear-regression" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Notes of Linear Regression</h2>
<p>The following notes are from this <a href="https://bookdown.org/rdpeng/advstatcomp/textbooks-vs-computers.html">BOOK</a>. Most of the original references become from there.</p>
<p>Pending,</p>
<ul>
<li>Expand the <strong>Guassian elimination</strong> notes with an analytical example and an <code>R</code> code</li>
<li>Expand the <strong>Gram-Schmidt process</strong> notes with an analytical example and an <code>R</code> code.</li>
</ul>
</div>
<div id="logarithms" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Logarithms</h2>
<p>Univariate normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span> is</p>
<p><span class="math inline">\(f(x|\mu, \sigma^2)=\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{1}{2\sigma^2}(x-\mu)^2}\)</span></p>
<p>and in <code>R</code> we can compute this value as</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="notes-of-optimization.html#cb4-1" aria-hidden="true"></a><span class="kw">dnorm</span>(<span class="dv">0</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>); <span class="kw">dnorm</span>(<span class="dv">0</span>)</span>
<span id="cb4-2"><a href="notes-of-optimization.html#cb4-2" aria-hidden="true"></a><span class="co">#&gt; [1] 0.3989423</span></span>
<span id="cb4-3"><a href="notes-of-optimization.html#cb4-3" aria-hidden="true"></a><span class="co">#&gt; [1] 0.3989423</span></span></code></pre></div>
<p>Calculating the equation, we have</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="notes-of-optimization.html#cb5-1" aria-hidden="true"></a><span class="co"># Example 1</span></span>
<span id="cb5-2"><a href="notes-of-optimization.html#cb5-2" aria-hidden="true"></a>UniNorm &lt;-<span class="st"> </span><span class="cf">function</span>(x, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sigma=</span><span class="dv">1</span>){</span>
<span id="cb5-3"><a href="notes-of-optimization.html#cb5-3" aria-hidden="true"></a>  <span class="co">#pi = 3.14159</span></span>
<span id="cb5-4"><a href="notes-of-optimization.html#cb5-4" aria-hidden="true"></a> output &lt;-<span class="st"> </span>(<span class="dv">1</span><span class="op">/</span><span class="kw">sqrt</span>(<span class="dv">2</span><span class="op">*</span>pi<span class="op">*</span>sigma))<span class="op">*</span><span class="kw">exp</span>(<span class="op">-</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">2</span><span class="op">*</span>(sigma)<span class="op">^</span><span class="dv">2</span>)<span class="op">*</span>((x<span class="op">-</span>mean)<span class="op">^</span><span class="dv">2</span>)) </span>
<span id="cb5-5"><a href="notes-of-optimization.html#cb5-5" aria-hidden="true"></a> <span class="kw">print</span>(output)</span>
<span id="cb5-6"><a href="notes-of-optimization.html#cb5-6" aria-hidden="true"></a>}</span>
<span id="cb5-7"><a href="notes-of-optimization.html#cb5-7" aria-hidden="true"></a><span class="kw">UniNorm</span>(<span class="dv">0</span>)</span>
<span id="cb5-8"><a href="notes-of-optimization.html#cb5-8" aria-hidden="true"></a><span class="kw">dnorm</span>(<span class="dv">0</span>)</span>
<span id="cb5-9"><a href="notes-of-optimization.html#cb5-9" aria-hidden="true"></a></span>
<span id="cb5-10"><a href="notes-of-optimization.html#cb5-10" aria-hidden="true"></a><span class="co"># Example 2</span></span>
<span id="cb5-11"><a href="notes-of-optimization.html#cb5-11" aria-hidden="true"></a><span class="kw">UniNorm</span>(<span class="dv">1</span>)</span>
<span id="cb5-12"><a href="notes-of-optimization.html#cb5-12" aria-hidden="true"></a><span class="kw">dnorm</span>(<span class="dv">1</span>)</span>
<span id="cb5-13"><a href="notes-of-optimization.html#cb5-13" aria-hidden="true"></a><span class="co">#&gt; [1] 0.3989423</span></span>
<span id="cb5-14"><a href="notes-of-optimization.html#cb5-14" aria-hidden="true"></a><span class="co">#&gt; [1] 0.3989423</span></span>
<span id="cb5-15"><a href="notes-of-optimization.html#cb5-15" aria-hidden="true"></a><span class="co">#&gt; [1] 0.2419707</span></span>
<span id="cb5-16"><a href="notes-of-optimization.html#cb5-16" aria-hidden="true"></a><span class="co">#&gt; [1] 0.2419707</span></span></code></pre></div>
<p>In practice, the exact number is not used, and the <span class="math inline">\(log\)</span> is used instead. However, in some cases we do need the value in the original scale. Calculating <em>densities</em> with <span class="math inline">\(log\)</span> is much more stable because when the exponential function is used the number become very small for the machine to represent (<em>underflow</em>), and if the ratio is used we could have also big numbers (<em>overflow</em>). The <span class="math inline">\(log\)</span> (and then the <span class="math inline">\(exp\)</span>) resolve some of these issues.</p>
</div>
<div id="linear-regression" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Linear Regression</h2>
<p>We express the linear regression in matrix form as,</p>
<p><span class="math inline">\(y = X \beta + \varepsilon\)</span></p>
<p>Were <span class="math inline">\(y\)</span> is a vector of size <span class="math inline">\(n \times 1\)</span> of observed response, <span class="math inline">\(X\)</span> is the <span class="math inline">\(n \times p\)</span> predictor matrix, <span class="math inline">\(\beta\)</span> is the <span class="math inline">\(p \times 1\)</span> coefficient vector, and <span class="math inline">\(\varepsilon\)</span> is the <span class="math inline">\(n \times 1\)</span> error vector.</p>
<p>To estimate <span class="math inline">\(\beta\)</span> (via maximum likelihood or least square), is often written in matrix form as</p>
<p><span class="math display">\[
\hat{\beta}=(X&#39;X)^{-1}X&#39;y
\]</span></p>
<p>Which could be estimated in <code>R</code> using <code>solve</code> to extract the inverse of the cross product matrix.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="notes-of-optimization.html#cb6-1" aria-hidden="true"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb6-2"><a href="notes-of-optimization.html#cb6-2" aria-hidden="true"></a>X &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">5000</span> <span class="op">*</span><span class="st"> </span><span class="dv">100</span>), <span class="dv">5000</span>, <span class="dv">100</span>)</span>
<span id="cb6-3"><a href="notes-of-optimization.html#cb6-3" aria-hidden="true"></a>y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">5000</span>)</span>
<span id="cb6-4"><a href="notes-of-optimization.html#cb6-4" aria-hidden="true"></a>betahat &lt;-<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>X) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>y</span>
<span id="cb6-5"><a href="notes-of-optimization.html#cb6-5" aria-hidden="true"></a><span class="kw">head</span>(betahat)</span>
<span id="cb6-6"><a href="notes-of-optimization.html#cb6-6" aria-hidden="true"></a><span class="co">#&gt;              [,1]</span></span>
<span id="cb6-7"><a href="notes-of-optimization.html#cb6-7" aria-hidden="true"></a><span class="co">#&gt; [1,]  0.013075906</span></span>
<span id="cb6-8"><a href="notes-of-optimization.html#cb6-8" aria-hidden="true"></a><span class="co">#&gt; [2,] -0.002368796</span></span>
<span id="cb6-9"><a href="notes-of-optimization.html#cb6-9" aria-hidden="true"></a><span class="co">#&gt; [3,] -0.007509733</span></span>
<span id="cb6-10"><a href="notes-of-optimization.html#cb6-10" aria-hidden="true"></a><span class="co">#&gt; [4,]  0.004884836</span></span>
<span id="cb6-11"><a href="notes-of-optimization.html#cb6-11" aria-hidden="true"></a><span class="co">#&gt; [5,]  0.011336186</span></span>
<span id="cb6-12"><a href="notes-of-optimization.html#cb6-12" aria-hidden="true"></a><span class="co">#&gt; [6,]  0.007056535</span></span></code></pre></div>
<p>Computationally, this is very expensive!</p>
<div class="alert alert-info">
<p>Quick look of <em>inverse matrixes</em> to understand the following equation:
<span class="math inline">\(\hat{\beta}=(X&#39;X)^{-1}X&#39;y\)</span></p>
<p>First, recall that not all square matrixes are inversible, and that there are some properties of the inverse matrix such as:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(AA^{-1}=I=A^{-1}A\)</span></li>
<li><span class="math inline">\((AB)^{-1}=B^{-1}A^{-1}\)</span></li>
<li><span class="math inline">\((A^{T})^{-1}=(A^{-1})^{T}\)</span></li>
</ol>
<p>Let use an example to disentangle some of the properties! First,</p>
<p><span class="math display">\[
A = 
\begin{pmatrix}
a &amp; b \\
c &amp; d
\end{pmatrix}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
A^{-1} = 
\begin{pmatrix}
x_{1} &amp; x_{2} \\
y_{1} &amp; y_{2}
\end{pmatrix}
\]</span></p>
<p>we also know that</p>
<p><span class="math display">\[
I = 
\begin{pmatrix}
1 &amp; 0 \\
0 &amp; 1
\end{pmatrix}
\]</span></p>
<p>All together and considering <span class="math inline">\((1)\)</span> we have</p>
<p><span class="math display">\[
AA^{-1}=I=A^{-1}A=
\begin{pmatrix}
a &amp; b \\
c &amp; d
\end{pmatrix}
\begin{pmatrix}
x_{1} &amp; x_{2} \\
y_{1} &amp; y_{2}
\end{pmatrix} =
\begin{pmatrix}
1 &amp; 0 \\
0 &amp; 1
\end{pmatrix} =
\begin{pmatrix}
x_{1} &amp; x_{2} \\
y_{1} &amp; y_{2}
\end{pmatrix}
\begin{pmatrix}
a &amp; b \\
c &amp; d
\end{pmatrix}
\]</span></p>
<p>Now, solving the <strong>linear system of equation</strong> we have</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(ax_{1}+by_{1}=1\)</span></li>
<li><span class="math inline">\(ax_{2}+by_{2}=0\)</span></li>
<li><span class="math inline">\(cx_{1}+dy_{1}=0\)</span></li>
<li><span class="math inline">\(cx_{2}+dy_{2}=1\)</span></li>
</ol>
<p>Also, re-arranging some of the terms and doing some <strong>elementary row operations</strong> we have for:</p>
<ol start="2" style="list-style-type: decimal">
<li><span class="math inline">\(ax_{2}+by_{2}=0\)</span></li>
</ol>
<p>That,</p>
<p><span class="math display">\[
y_{2}=\frac{-a}{b}x_{2}
\]</span></p>
<p>and</p>
<ol start="3" style="list-style-type: decimal">
<li><span class="math inline">\(cx_{1}+dy_{1}=0\)</span></li>
</ol>
<p><span class="math display">\[
y_{1}=\frac{-c}{d}x_{1}
\]</span></p>
<p>Now,</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(ax_{1}+by_{1}=1\)</span></li>
</ol>
<p>we could replace some terms in such a way that,</p>
<p><span class="math display">\[
ax_{1}-\frac{bc}{d}x_{1}=1
\]</span></p>
<p>and</p>
<p><span class="math display">\[
x_{1}=\frac{d}{ad-bc}
\]</span></p>
<p>then</p>
<p><span class="math display">\[
y_{1}=\frac{-c}{ad-bc}
\]</span></p>
<p>Also,</p>
<p><span class="math display">\[
\frac{c}{b}x_{2}-\frac{ad}{b}x_{2}=1
\]</span></p>
<p>is expressed as</p>
<p><span class="math display">\[
x_{2}=\frac{b}{bc-ad}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
y_{2}=\frac{a}{ad-bc}
\]</span></p>
<p>Puting all together we have the inverse of the matrix</p>
<p><span class="math display">\[
A^{-1}= \frac{1}{ad-bc}
\begin{pmatrix}
d &amp; -b \\
-c &amp; a
\end{pmatrix}
\]</span></p>
<p>And, considering that the determinant is,</p>
<p><span class="math display">\[
|A| = 
\begin{vmatrix}
a &amp; b \\
c &amp; d
\end{vmatrix} = 
ad-bc
\]</span></p>
<p>If the resulting value of <span class="math inline">\(ad-bc=0\)</span>, then the matrix is not invertible (is singular or degenerate)</p>
<p>In <code>R</code> all this calculation is just a simple function. For example,</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="notes-of-optimization.html#cb7-1" aria-hidden="true"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb7-2"><a href="notes-of-optimization.html#cb7-2" aria-hidden="true"></a>X &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">100</span> <span class="op">*</span><span class="st"> </span><span class="dv">100</span>), <span class="dv">100</span>, <span class="dv">100</span>)</span>
<span id="cb7-3"><a href="notes-of-optimization.html#cb7-3" aria-hidden="true"></a>inv_X &lt;-<span class="st"> </span><span class="kw">solve</span>(X)</span></code></pre></div>
</div>
<p>A better option that is less computationally demanding is re-arranging the terms in the following way:</p>
<p><span class="math display">\[
X&#39;X\beta = X&#39;y
\]</span></p>
<p>Which, gives the same result</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="notes-of-optimization.html#cb8-1" aria-hidden="true"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb8-2"><a href="notes-of-optimization.html#cb8-2" aria-hidden="true"></a>X &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">5000</span> <span class="op">*</span><span class="st"> </span><span class="dv">100</span>), <span class="dv">5000</span>, <span class="dv">100</span>)</span>
<span id="cb8-3"><a href="notes-of-optimization.html#cb8-3" aria-hidden="true"></a>betahat &lt;-<span class="st"> </span><span class="kw">solve</span>(<span class="kw">crossprod</span>(X), <span class="kw">crossprod</span>(X, y))</span>
<span id="cb8-4"><a href="notes-of-optimization.html#cb8-4" aria-hidden="true"></a><span class="kw">head</span>(betahat)</span>
<span id="cb8-5"><a href="notes-of-optimization.html#cb8-5" aria-hidden="true"></a><span class="co">#&gt;              [,1]</span></span>
<span id="cb8-6"><a href="notes-of-optimization.html#cb8-6" aria-hidden="true"></a><span class="co">#&gt; [1,]  0.013075906</span></span>
<span id="cb8-7"><a href="notes-of-optimization.html#cb8-7" aria-hidden="true"></a><span class="co">#&gt; [2,] -0.002368796</span></span>
<span id="cb8-8"><a href="notes-of-optimization.html#cb8-8" aria-hidden="true"></a><span class="co">#&gt; [3,] -0.007509733</span></span>
<span id="cb8-9"><a href="notes-of-optimization.html#cb8-9" aria-hidden="true"></a><span class="co">#&gt; [4,]  0.004884836</span></span>
<span id="cb8-10"><a href="notes-of-optimization.html#cb8-10" aria-hidden="true"></a><span class="co">#&gt; [5,]  0.011336186</span></span>
<span id="cb8-11"><a href="notes-of-optimization.html#cb8-11" aria-hidden="true"></a><span class="co">#&gt; [6,]  0.007056535</span></span></code></pre></div>
<div id="gaussian-elimination" class="section level3" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Gaussian elimination</h3>
<p>The difference between computing the inverse of <span class="math inline">\(X&#39;X\)</span>, and using a <em>Gaussian elimination</em> to compute <span class="math inline">\(\hat{\beta}\)</span> is that the solution is numerically more stable and faster. Also, if there are high colinearity amongst the predictors, then the results would be unstable if the inverse in <span class="math inline">\(X&#39;X\)</span> is used.</p>
<div class="alert alert-info">
<p><strong>PENDING!!!! (check my algebra notebook)</strong></p>
<p>Quick look of <strong>Guassian elimination</strong> (or <strong>row reduction</strong>): algorithm to solve linear equations.</p>
<p>The idea is to use <strong>elementary row operations</strong> and modify te matrix to produce a “triangular” matrix with zeros in the bottom left corner (achieving a <strong>upper triangular matrix</strong>), that is said to be in a <strong>reduced row echelon form</strong>.</p>
<p>For example, we could try the <strong>back substitution</strong> in an augmented matrix</p>
<p><strong>PENDING: edit the matrix R1(ccc|c)</strong>
<span class="math display">\[
\begin{array}{ccc|c} 
1 &amp; -1 &amp; 5 &amp; -9 \\ 
2 &amp; -1 &amp; -3 &amp; -19 \\ 
3 &amp; 1 &amp; 4 &amp; -13
\end{array} 
\]</span></p>
<p>Consiering that we have three rows (<span class="math inline">\(R1, R2, R3\)</span>)…</p>
<p><strong>I JUST ADD HERE A NICE MATRIX!!!:</strong></p>
<p><span class="math display">\[
A = 
  \begin{matrix}\begin{pmatrix}x &amp; y\end{pmatrix}\\\mbox{}\end{matrix}
  \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} 
  \begin{pmatrix} x \\ y \end{pmatrix}
\]</span></p>
<p><a href="https://stackoverflow.com/questions/16044377/how-to-do-gaussian-elimination-in-r-do-not-use-solve">here</a></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="notes-of-optimization.html#cb9-1" aria-hidden="true"></a><span class="co"># Data</span></span>
<span id="cb9-2"><a href="notes-of-optimization.html#cb9-2" aria-hidden="true"></a>A &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">2</span>,<span class="op">-</span><span class="dv">5</span>,<span class="dv">4</span>,<span class="dv">1</span>,<span class="op">-</span><span class="fl">2.5</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="op">-</span><span class="dv">4</span>,<span class="dv">6</span>),<span class="dt">byrow=</span>T,<span class="dt">nrow=</span><span class="dv">3</span>,<span class="dt">ncol=</span><span class="dv">3</span>)</span>
<span id="cb9-3"><a href="notes-of-optimization.html#cb9-3" aria-hidden="true"></a>b &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">10</span>),<span class="dt">nrow=</span><span class="dv">3</span>,<span class="dt">ncol=</span><span class="dv">1</span>)</span>
<span id="cb9-4"><a href="notes-of-optimization.html#cb9-4" aria-hidden="true"></a>p &lt;-<span class="st"> </span><span class="kw">nrow</span>(A)</span>
<span id="cb9-5"><a href="notes-of-optimization.html#cb9-5" aria-hidden="true"></a>(U.pls &lt;-<span class="st"> </span><span class="kw">cbind</span>(A,b))</span>
<span id="cb9-6"><a href="notes-of-optimization.html#cb9-6" aria-hidden="true"></a></span>
<span id="cb9-7"><a href="notes-of-optimization.html#cb9-7" aria-hidden="true"></a><span class="co"># Gaussian Elimination</span></span>
<span id="cb9-8"><a href="notes-of-optimization.html#cb9-8" aria-hidden="true"></a>U.pls[<span class="dv">1</span>,] &lt;-<span class="st"> </span>U.pls[<span class="dv">1</span>,]<span class="op">/</span>U.pls[<span class="dv">1</span>,<span class="dv">1</span>]</span>
<span id="cb9-9"><a href="notes-of-optimization.html#cb9-9" aria-hidden="true"></a>i &lt;-<span class="st"> </span><span class="dv">2</span></span>
<span id="cb9-10"><a href="notes-of-optimization.html#cb9-10" aria-hidden="true"></a><span class="cf">while</span> (i <span class="op">&lt;</span><span class="st"> </span>p<span class="op">+</span><span class="dv">1</span>) {</span>
<span id="cb9-11"><a href="notes-of-optimization.html#cb9-11" aria-hidden="true"></a> j &lt;-<span class="st"> </span>i</span>
<span id="cb9-12"><a href="notes-of-optimization.html#cb9-12" aria-hidden="true"></a> <span class="cf">while</span> (j <span class="op">&lt;</span><span class="st"> </span>p<span class="op">+</span><span class="dv">1</span>) {</span>
<span id="cb9-13"><a href="notes-of-optimization.html#cb9-13" aria-hidden="true"></a>  U.pls[j, ] &lt;-<span class="st"> </span>U.pls[j, ] <span class="op">-</span><span class="st"> </span>U.pls[i<span class="dv">-1</span>, ] <span class="op">*</span><span class="st"> </span>U.pls[j, i<span class="dv">-1</span>]</span>
<span id="cb9-14"><a href="notes-of-optimization.html#cb9-14" aria-hidden="true"></a>  j &lt;-<span class="st"> </span>j<span class="op">+</span><span class="dv">1</span></span>
<span id="cb9-15"><a href="notes-of-optimization.html#cb9-15" aria-hidden="true"></a> }</span>
<span id="cb9-16"><a href="notes-of-optimization.html#cb9-16" aria-hidden="true"></a> <span class="cf">while</span> (U.pls[i,i] <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) {</span>
<span id="cb9-17"><a href="notes-of-optimization.html#cb9-17" aria-hidden="true"></a>  U.pls &lt;-<span class="st"> </span><span class="kw">rbind</span>(U.pls[<span class="op">-</span>i,],U.pls[i,])</span>
<span id="cb9-18"><a href="notes-of-optimization.html#cb9-18" aria-hidden="true"></a> }</span>
<span id="cb9-19"><a href="notes-of-optimization.html#cb9-19" aria-hidden="true"></a> U.pls[i,] &lt;-<span class="st"> </span>U.pls[i,]<span class="op">/</span>U.pls[i,i]</span>
<span id="cb9-20"><a href="notes-of-optimization.html#cb9-20" aria-hidden="true"></a> i &lt;-<span class="st"> </span>i<span class="op">+</span><span class="dv">1</span></span>
<span id="cb9-21"><a href="notes-of-optimization.html#cb9-21" aria-hidden="true"></a>}</span>
<span id="cb9-22"><a href="notes-of-optimization.html#cb9-22" aria-hidden="true"></a><span class="cf">for</span> (i <span class="cf">in</span> p<span class="op">:</span><span class="dv">2</span>){</span>
<span id="cb9-23"><a href="notes-of-optimization.html#cb9-23" aria-hidden="true"></a> <span class="cf">for</span> (j <span class="cf">in</span> i<span class="op">:</span><span class="dv">2-1</span>) {</span>
<span id="cb9-24"><a href="notes-of-optimization.html#cb9-24" aria-hidden="true"></a>  U.pls[j, ] &lt;-<span class="st"> </span>U.pls[j, ] <span class="op">-</span><span class="st"> </span>U.pls[i, ] <span class="op">*</span><span class="st"> </span>U.pls[j, i]</span>
<span id="cb9-25"><a href="notes-of-optimization.html#cb9-25" aria-hidden="true"></a> }</span>
<span id="cb9-26"><a href="notes-of-optimization.html#cb9-26" aria-hidden="true"></a>}</span>
<span id="cb9-27"><a href="notes-of-optimization.html#cb9-27" aria-hidden="true"></a>U.pls</span>
<span id="cb9-28"><a href="notes-of-optimization.html#cb9-28" aria-hidden="true"></a></span>
<span id="cb9-29"><a href="notes-of-optimization.html#cb9-29" aria-hidden="true"></a><span class="co"># Check:</span></span>
<span id="cb9-30"><a href="notes-of-optimization.html#cb9-30" aria-hidden="true"></a><span class="kw">library</span>(pracma)</span>
<span id="cb9-31"><a href="notes-of-optimization.html#cb9-31" aria-hidden="true"></a><span class="kw">rref</span>(<span class="kw">cbind</span>(A, b))</span>
<span id="cb9-32"><a href="notes-of-optimization.html#cb9-32" aria-hidden="true"></a><span class="co">#&gt;      [,1] [,2] [,3] [,4]</span></span>
<span id="cb9-33"><a href="notes-of-optimization.html#cb9-33" aria-hidden="true"></a><span class="co">#&gt; [1,]    2 -5.0    4   -3</span></span>
<span id="cb9-34"><a href="notes-of-optimization.html#cb9-34" aria-hidden="true"></a><span class="co">#&gt; [2,]    1 -2.5    1    5</span></span>
<span id="cb9-35"><a href="notes-of-optimization.html#cb9-35" aria-hidden="true"></a><span class="co">#&gt; [3,]    1 -4.0    6   10</span></span>
<span id="cb9-36"><a href="notes-of-optimization.html#cb9-36" aria-hidden="true"></a><span class="co">#&gt;      [,1] [,2] [,3]  [,4]</span></span>
<span id="cb9-37"><a href="notes-of-optimization.html#cb9-37" aria-hidden="true"></a><span class="co">#&gt; [1,]    1    0    0 -51.0</span></span>
<span id="cb9-38"><a href="notes-of-optimization.html#cb9-38" aria-hidden="true"></a><span class="co">#&gt; [2,]    0    1    0 -25.0</span></span>
<span id="cb9-39"><a href="notes-of-optimization.html#cb9-39" aria-hidden="true"></a><span class="co">#&gt; [3,]    0    0    1  -6.5</span></span>
<span id="cb9-40"><a href="notes-of-optimization.html#cb9-40" aria-hidden="true"></a><span class="co">#&gt;      [,1] [,2] [,3]  [,4]</span></span>
<span id="cb9-41"><a href="notes-of-optimization.html#cb9-41" aria-hidden="true"></a><span class="co">#&gt; [1,]    1    0    0 -51.0</span></span>
<span id="cb9-42"><a href="notes-of-optimization.html#cb9-42" aria-hidden="true"></a><span class="co">#&gt; [2,]    0    1    0 -25.0</span></span>
<span id="cb9-43"><a href="notes-of-optimization.html#cb9-43" aria-hidden="true"></a><span class="co">#&gt; [3,]    0    0    1  -6.5</span></span></code></pre></div>
</div>
<p>Comparing both strategies, we could check that the <strong>Gaussian elimination</strong> in comparisson with the other strategy is less time consuming</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="notes-of-optimization.html#cb10-1" aria-hidden="true"></a><span class="kw">library</span>(microbenchmark)</span>
<span id="cb10-2"><a href="notes-of-optimization.html#cb10-2" aria-hidden="true"></a><span class="kw">library</span>(magrittr)</span>
<span id="cb10-3"><a href="notes-of-optimization.html#cb10-3" aria-hidden="true"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb10-4"><a href="notes-of-optimization.html#cb10-4" aria-hidden="true"></a>X &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">5000</span> <span class="op">*</span><span class="st"> </span><span class="dv">100</span>), <span class="dv">5000</span>, <span class="dv">100</span>)</span>
<span id="cb10-5"><a href="notes-of-optimization.html#cb10-5" aria-hidden="true"></a><span class="kw">microbenchmark</span>(</span>
<span id="cb10-6"><a href="notes-of-optimization.html#cb10-6" aria-hidden="true"></a>  <span class="dt">inverse =</span> <span class="kw">solve</span>(<span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>X) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>y,</span>
<span id="cb10-7"><a href="notes-of-optimization.html#cb10-7" aria-hidden="true"></a>  <span class="dt">gaussian =</span> <span class="kw">solve</span>(<span class="kw">crossprod</span>(X), <span class="kw">crossprod</span>(X, y))</span>
<span id="cb10-8"><a href="notes-of-optimization.html#cb10-8" aria-hidden="true"></a>  ) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summary</span>(<span class="dt">unit =</span> <span class="st">&quot;ms&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span>knitr<span class="op">::</span><span class="kw">kable</span>(<span class="dt">format =</span> <span class="st">&quot;markdown&quot;</span>)</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">expr</th>
<th align="right">min</th>
<th align="right">lq</th>
<th align="right">mean</th>
<th align="right">median</th>
<th align="right">uq</th>
<th align="right">max</th>
<th align="right">neval</th>
<th align="left">cld</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">inverse</td>
<td align="right">85.75258</td>
<td align="right">93.64495</td>
<td align="right">103.03926</td>
<td align="right">98.08822</td>
<td align="right">105.50272</td>
<td align="right">263.36540</td>
<td align="right">100</td>
<td align="left">b</td>
</tr>
<tr class="even">
<td align="left">gaussian</td>
<td align="right">27.93382</td>
<td align="right">28.76065</td>
<td align="right">31.27557</td>
<td align="right">29.12637</td>
<td align="right">30.31611</td>
<td align="right">58.34899</td>
<td align="right">100</td>
<td align="left">a</td>
</tr>
</tbody>
</table>
<p>On the other hand, the Gaussian elimination would breaks down when there is collinearity in the <span class="math inline">\(X\)</span> matrix. Meaning that the column <span class="math inline">\(X\)</span> would be very similar, but not identical, to the first column of <span class="math inline">\(X\)</span>. For example,</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="notes-of-optimization.html#cb11-1" aria-hidden="true"></a><span class="kw">set.seed</span>(<span class="dv">127893</span>)</span>
<span id="cb11-2"><a href="notes-of-optimization.html#cb11-2" aria-hidden="true"></a>X &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">5000</span> <span class="op">*</span><span class="st"> </span><span class="dv">100</span>), <span class="dv">5000</span>, <span class="dv">100</span>)</span>
<span id="cb11-3"><a href="notes-of-optimization.html#cb11-3" aria-hidden="true"></a>W &lt;-<span class="st"> </span><span class="kw">cbind</span>(X, X[, <span class="dv">1</span>] <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">5000</span>, <span class="dt">sd =</span> <span class="fl">0.0000000001</span>))</span>
<span id="cb11-4"><a href="notes-of-optimization.html#cb11-4" aria-hidden="true"></a><span class="kw">solve</span>(<span class="kw">crossprod</span>(W), <span class="kw">crossprod</span>(W, y)) </span>
<span id="cb11-5"><a href="notes-of-optimization.html#cb11-5" aria-hidden="true"></a><span class="co">#&gt; Error in solve.default(crossprod(W), crossprod(W, y)): system is computationally singular: reciprocal condition number = 1.47235e-16</span></span></code></pre></div>
<p>In this case, the cross product matrix <span class="math inline">\(W&#39;W\)</span> is singular (determinant is <span class="math inline">\(0\)</span>)</p>
</div>
<div id="qr-decomposition" class="section level3" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> QR decomposition</h3>
<p><code>R</code> use as a default the <strong>QR decomposition</strong>, that is not fast, but can detect and handle colinear columns in the matrix.</p>
<div class="alert alert-info">
<p>Quick look of <em>orthogonal matrix</em> to understand the <strong>QR decomposition</strong> (also known as <strong>QR factorization</strong> or <strong>QU factorization</strong>): <span class="math inline">\(A=QR\)</span>.</p>
<p>To work out with the orthogonal vector, we often work with <strong>orthonormal</strong> vectors. That assumes that all the vectors have lenght <span class="math inline">\(1\)</span> (<span class="math inline">\(||v_{1}||=1\)</span>, <span class="math inline">\(||v_{1}||^{2}=1\)</span> or <span class="math inline">\(v_{1}v_{2}=1\)</span> for <span class="math inline">\(i, 1,2,...k\)</span>). Therefore, they have all been “normalized” (unit vectors).</p>
<p>Two vectors <span class="math inline">\(v_{1}\)</span> and <span class="math inline">\(v_{2}\)</span>, are said to be <em>orthogonal</em> if <span class="math inline">\(\langle v_{1},v_{2} \rangle = 0\)</span> (sometime expressed as <span class="math inline">\(v_{1} \perp v_{2}\)</span>)</p>
<p>A set of nonzero vectors that are mutually orthogonal are necessarily linearly independent. Meaning that each vector in one is orthogonal to every vector in the other, and said to be <em>normal</em> to that space (<em>normal vector</em>).</p>
<p><a href="https://www.khanacademy.org/math/linear-algebra/alternate-bases/orthonormal-basis/v/linear-algebra-introduction-to-orthonormal-bases">Example</a>:</p>
<p>Assuming that we have a vector</p>
<p><span class="math display">\[
v_{1} = 
  \begin{pmatrix} 1/3 \\ 2/3 \\ 2/3 \end{pmatrix}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
v_{1} = 
  \begin{pmatrix} 2/3 \\ 1/3 \\ -2/3 \end{pmatrix}
\]</span></p>
<p>and <span class="math inline">\(B=\{v_{1}, v_{2}\}\)</span>.</p>
<p>What is the lenght of <span class="math inline">\(v_{1}\)</span> and <span class="math inline">\(v_{2}\)</span>?</p>
<p><span class="math inline">\(||v_{1}||^2=v_{1}v_{2}=1/9+4/9+4/9=1\)</span> and <span class="math inline">\(||v_{2}||^2=4/9+1/9+4/9=1\)</span>. We know that we have a normalized set <span class="math inline">\(B\)</span>.</p>
<p>Are they orthogonal?
<span class="math inline">\(v_{1}v_{2}=2/9+2/9+-4/9=0\)</span></p>
<p>If we know that we have a space, such as <span class="math inline">\(V=span(v_{1},v_{2})\)</span>, the we can say that <span class="math inline">\(B\)</span> is an ortohonormal basis for <span class="math inline">\(V\)</span>.</p>
<p>We do know that the <strong>QR decomposition</strong> decompose a matrix <span class="math inline">\(A\)</span> into a product <span class="math inline">\(A=QR\)</span> of an othogonal matrix <span class="math inline">\(Q\)</span> and an upper triangular matrix <span class="math inline">\(R\)</span>.</p>
</div>
<div class="alert alert-info">
<p>Quick look of the <a href="https://en.wikipedia.org/wiki/QR_decomposition"><strong>Gram-Schmidt process</strong></a> to compute the <strong>QR decomposition</strong>.</p>
<p><strong>PENDING!!!! (check my algebra notebook)</strong></p>
<p><a href="https://genomicsclass.github.io/book/pages/qr_and_regression.html">check</a></p>
</div>
<p>Knowing that <span class="math inline">\(X\)</span> can be decomposed as <span class="math inline">\(X=QR\)</span>, the linear regression</p>
<p><span class="math display">\[
X&#39;X\beta = X&#39;y
\]</span></p>
<p>can be writted as</p>
<p><span class="math display">\[
R&#39;Q&#39;QR&#39;\beta = R&#39;Q&#39;y \\
R&#39;R\beta = R&#39;Q&#39;y \\
R\beta = Q&#39;y
\]</span></p>
<p>Considering that <span class="math inline">\(Q&#39;Q=I\)</span>, now we have a simpler equation that does not longer require to compute the cross product. Also, due the QR decomposition <span class="math inline">\(R\)</span> is upper triangular and, therefore, we can solve <span class="math inline">\(\beta\)</span> via Gaussian elimination. Some of the benefits are that the cross product <span class="math inline">\(X&#39;X\)</span> was numerically unstable if it is not properly centered or scaled.</p>
<p>To compute the singular matrix <span class="math inline">\(W\)</span> of the example, the QR decomposition continous without error. Notices that the ouput have <span class="math inline">\(100\)</span> and not <span class="math inline">\(101\)</span> ranks, this is because the colinear column.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="notes-of-optimization.html#cb12-1" aria-hidden="true"></a>Qw &lt;-<span class="st"> </span><span class="kw">qr</span>(W)</span>
<span id="cb12-2"><a href="notes-of-optimization.html#cb12-2" aria-hidden="true"></a><span class="kw">str</span>(Qw)</span>
<span id="cb12-3"><a href="notes-of-optimization.html#cb12-3" aria-hidden="true"></a><span class="co">#&gt; List of 4</span></span>
<span id="cb12-4"><a href="notes-of-optimization.html#cb12-4" aria-hidden="true"></a><span class="co">#&gt;  $ qr   : num [1:5000, 1:101] -7.04e+01 -1.15e-02 -8.27e-05 6.89e-03 -1.13e-02 ...</span></span>
<span id="cb12-5"><a href="notes-of-optimization.html#cb12-5" aria-hidden="true"></a><span class="co">#&gt;  $ rank : int 100</span></span>
<span id="cb12-6"><a href="notes-of-optimization.html#cb12-6" aria-hidden="true"></a><span class="co">#&gt;  $ qraux: num [1:101] 1.03 1.01 1.03 1.01 1.02 ...</span></span>
<span id="cb12-7"><a href="notes-of-optimization.html#cb12-7" aria-hidden="true"></a><span class="co">#&gt;  $ pivot: int [1:101] 1 2 3 4 5 6 7 8 9 10 ...</span></span>
<span id="cb12-8"><a href="notes-of-optimization.html#cb12-8" aria-hidden="true"></a><span class="co">#&gt;  - attr(*, &quot;class&quot;)= chr &quot;qr&quot;</span></span></code></pre></div>
<p>After understanding the QR decomposition for the matrix, we can now solve the regression equation to estimate <span class="math inline">\(\hat{\beta}\)</span> using <code>R</code>:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="notes-of-optimization.html#cb13-1" aria-hidden="true"></a>betahat &lt;-<span class="st"> </span><span class="kw">qr.coef</span>(Qw, y)</span>
<span id="cb13-2"><a href="notes-of-optimization.html#cb13-2" aria-hidden="true"></a><span class="kw">tail</span>(betahat, <span class="dv">1</span>)</span>
<span id="cb13-3"><a href="notes-of-optimization.html#cb13-3" aria-hidden="true"></a><span class="co">#&gt; [1] NA</span></span></code></pre></div>
<p>Notices that the last element in the position <span class="math inline">\(101\)</span> is <code>NA</code> due the colliniarity. Meaning that the coefficient could not be calculated.</p>
<p>This approach helps with colliniarity, is better and more stable. However, is slower:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="notes-of-optimization.html#cb14-1" aria-hidden="true"></a><span class="kw">library</span>(ggplot2)</span>
<span id="cb14-2"><a href="notes-of-optimization.html#cb14-2" aria-hidden="true"></a><span class="kw">library</span>(microbenchmark)</span>
<span id="cb14-3"><a href="notes-of-optimization.html#cb14-3" aria-hidden="true"></a>m &lt;-<span class="st"> </span><span class="kw">microbenchmark</span>(<span class="kw">solve</span>(<span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>X) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>y,</span>
<span id="cb14-4"><a href="notes-of-optimization.html#cb14-4" aria-hidden="true"></a>                    <span class="kw">solve</span>(<span class="kw">crossprod</span>(X), <span class="kw">crossprod</span>(X, y)),</span>
<span id="cb14-5"><a href="notes-of-optimization.html#cb14-5" aria-hidden="true"></a>                    <span class="kw">qr.coef</span>(<span class="kw">qr</span>(X), y))</span>
<span id="cb14-6"><a href="notes-of-optimization.html#cb14-6" aria-hidden="true"></a><span class="kw">autoplot</span>(m)</span></code></pre></div>
<p><img src="man/figures/README-unnamed-chunk-12-1.png" width="100%" /></p>
</div>
</div>
<div id="multivariate-normal-distribution" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Multivariate Normal Distribution</h2>
<p>The <em>p</em>-dimensional multivariate Normal density is written as
<span class="math display">\[
\begin{aligned}
\varphi(x|\mu, \Sigma)=-\frac{p}{2}log|\Sigma|-\frac{1}{2}(x-\mu)&#39;\Sigma ^{ -1}(x-\mu)
\end{aligned}
\]</span></p>
<div class="alert alert-info">
<p>Quick look of the <strong>multivariate Normal density</strong>.</p>
<p>Considering that the Gaussian or normal distribution for the univariate case is <span class="math inline">\(f(x|\mu, \sigma^2)=\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{1}{2\sigma^2}(x-\mu)^2}\)</span></p>
<p>With parameters: mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^{2}\)</span> (standard deviation <span class="math inline">\(\sigma\)</span>). We know that the maximum likelihood estimates are
<span class="math display">\[
\begin{aligned}
\hat{\mu}=\frac{1}{N}\sum_{i}x^{(i)} \\
\hat{\sigma}^{2}=\frac{1}{N}\sum_{i}(x^{(i)}-\hat{\mu})^{2}
\end{aligned}
\]</span></p>
<p>Then, we have the multivariate Normal density which is the extension of this model to vector value random variables in a multidimensional space. In which <span class="math inline">\(x\)</span> would be a vector with <span class="math inline">\(d\)</span> values, with <span class="math inline">\(\mu\)</span> as the length-d row vector, and <span class="math inline">\(\Sigma\)</span> a <span class="math inline">\(d \times d\)</span> matrix.Remember that <span class="math inline">\(|\Sigma|\)</span> is the determinant matrix of the covariants,</p>
<p>The maximum likelihood are similar to the univariate case
<span class="math display">\[
\begin{aligned}
\hat{\mu}=\frac{1}{m}\sum_{j}x^{(j)} \\
\hat{\Sigma}^{2}=\frac{1}{m}\sum_{j}(x^{(j)}-\hat{\mu})^{T}(x^{(j)}-\hat{\mu})
\end{aligned}
\]</span>
Where <span class="math inline">\(\Sigma\)</span> is the average of the <span class="math inline">\(d \times d\)</span> matrix (outer product).</p>
<p>For <a href="https://www.youtube.com/watch?v=eho8xH3E6mE">example</a>,</p>
<p>If we have two independent Gaussian variables <span class="math inline">\(x_{1}\)</span> and <span class="math inline">\(x_{2}\)</span>, normalized with a <span class="math inline">\(Z\)</span> constant
<span class="math display">\[
\begin{aligned}
p(x_{1})=\frac{1}{Z_{1}}exp\{-\frac{1}{2\sigma^{2}_{1}}(x_{1}-\mu_{1})^{2}\} \\
p(x_{2})=\frac{1}{Z_{2}}exp\{-\frac{1}{2\sigma^{2}_{2}}(x_{2}-\mu_{2})^{2}\}
\end{aligned}
\]</span>
We can a new vector concatenating the two vectors, <span class="math inline">\(x=[x_{1}x_{2}]\)</span> we can ask for the distribution of <span class="math inline">\(x\)</span> assuming that <span class="math inline">\(x_{1}\)</span> and <span class="math inline">\(x_{2}\)</span> are independents. Then, the joint distribution is the product of the individual distributions. Then, we have
<span class="math display">\[
p(x_{1})p(x_{2})=\frac{1}{Z_{1}Z_{2}}exp\{-\frac{1}{2}\}(x-\mu)^{T}\Sigma^{-1}(x-\mu) \\
\mu=[\mu_{1}\mu_{2}] \\
\Sigma = diag(\sigma^{2}_{1}, \sigma^{2}_{2}) \\
\Sigma=\begin{pmatrix} \sigma^{2}_{11} &amp; 0 \\ 0 &amp; \sigma^{2}_{22} \end{pmatrix}
\]</span></p>
</div>
<p>From the multivariate Normal density equation, the most time-consuming part is the quadratic form
<span class="math display">\[
(x-\mu)&#39;\Sigma^{-1}(x-\mu) = z&#39;\Sigma^{-1}z \\
z=x-\mu
\]</span></p>
<p>Similarly to the regression example above is the inversion of the of the <em>p</em>-dimensional covariance matrix <span class="math inline">\(\Sigma\)</span>. Taking <span class="math inline">\(z\)</span> as a <span class="math inline">\(p \times 1\)</span> column vector, then in <code>R</code> this could be expressed as</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="notes-of-optimization.html#cb15-1" aria-hidden="true"></a><span class="kw">t</span>(z) <span class="op">%*%</span><span class="st"> </span><span class="kw">solve</span>(Sigma) <span class="op">%*%</span><span class="st"> </span>z</span></code></pre></div>
<div id="choleskey-decomposition" class="section level3" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Choleskey decomposition</h3>
<p>Rather than using the literal equation, a similar approach is using the <strong>Choleskey decomposition</strong> of <span class="math inline">\(\Sigma\)</span>.</p>
<div class="alert alert-info">
<p>Quick look of the <strong>Choleskey decomposition</strong> or <strong>Choleskey factorization</strong>.</p>
<p><a href="https://www.maths.manchester.ac.uk/~higham/papers/high09c.pdf">here</a>
Recall that if all the eigenvalues of <span class="math inline">\(A\)</span> are positive, or if <span class="math inline">\(x^{T}Ax\)</span> is positive for all non-zero vector <span class="math inline">\(x\)</span> (which is equivalent), then we assume that a symmetric <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(A\)</span> is <strong>positive definite</strong>. If a matrix is <strong>positive definite</strong>, then it could be defined as <span class="math inline">\(A=X&#39;X\)</span> for a non-singular (non-invertible) matrix <span class="math inline">\(X\)</span>.</p>
<p><strong>PENDING!!!! (check my algebra notebook)</strong></p>
<p><a href="http://www.seas.ucla.edu/~vandenbe/133A/lectures/chol.pdf">check</a></p>
</div>
<div class="alert alert-info">
<p>Quick look of the <strong>eigenvalues</strong>.</p>
<p><strong>PENDING!!!! (check my algebra notebook)</strong></p>
</div>
<p>Using the Choleskey decomposition for a positive definite matrix on <span class="math inline">\(\Sigma\)</span>, we have
<span class="math display">\[
\Sigma = R&#39;R 
\]</span>
were <span class="math inline">\(R\)</span> is an upper triangular matrix (also called the <em>square root</em> of <span class="math inline">\(\Sigma\)</span>). Using Choleskey decomposition on <span class="math inline">\(\Sigma\)</span> and the rules of matrix algebra, the multivariate Normal density equation can be written as
<span class="math display">\[
z&#39;\Sigma^{-1}z = z&#39;(R&#39;R)^{-1}z \\
= z&#39;R^{-1}R&#39;^{-1}z \\
= (R&#39;^{-1}z)&#39;R&#39;^{-1}z \\
= v&#39;v
\]</span>
Where <span class="math inline">\(v=R&#39;^{-1}z\)</span> and is a <span class="math inline">\(p \times 1\)</span> vector. Also, to avoid inverting <span class="math inline">\(R&#39;^{-1}\)</span> by computing <span class="math inline">\(v\)</span> as solution of the linear system
<span class="math display">\[
R&#39;v=z
\]</span>
Then, computing <span class="math inline">\(v\)</span>, we can compute <span class="math inline">\(v&#39;v\)</span>, which is simply the cross-product of two <em>p</em>-dimensional vectors.</p>
<p>One of the benefits of using Choleskey decomposition is that gives a way of computing the log-determinant of <span class="math inline">\(\Sigma\)</span>. Where the log-determinant of <span class="math inline">\(\Sigma\)</span> is simply <span class="math inline">\(2\)</span> times the sum of the log of the diagional elements of <span class="math inline">\(R\)</span>. Implementing this in a function</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="notes-of-optimization.html#cb16-1" aria-hidden="true"></a><span class="kw">set.seed</span>(<span class="dv">93287</span>)</span>
<span id="cb16-2"><a href="notes-of-optimization.html#cb16-2" aria-hidden="true"></a>z &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">200</span> <span class="op">*</span><span class="st"> </span><span class="dv">100</span>), <span class="dv">200</span>, <span class="dv">100</span>)</span>
<span id="cb16-3"><a href="notes-of-optimization.html#cb16-3" aria-hidden="true"></a>S &lt;-<span class="st"> </span><span class="kw">cov</span>(z)</span>
<span id="cb16-4"><a href="notes-of-optimization.html#cb16-4" aria-hidden="true"></a>quad.naive &lt;-<span class="st"> </span><span class="cf">function</span>(z, S) {</span>
<span id="cb16-5"><a href="notes-of-optimization.html#cb16-5" aria-hidden="true"></a>        Sinv &lt;-<span class="st"> </span><span class="kw">solve</span>(S)</span>
<span id="cb16-6"><a href="notes-of-optimization.html#cb16-6" aria-hidden="true"></a>        <span class="kw">rowSums</span>((z <span class="op">%*%</span><span class="st"> </span>Sinv) <span class="op">*</span><span class="st"> </span>z)</span>
<span id="cb16-7"><a href="notes-of-optimization.html#cb16-7" aria-hidden="true"></a>}</span>
<span id="cb16-8"><a href="notes-of-optimization.html#cb16-8" aria-hidden="true"></a><span class="kw">library</span>(dplyr)</span>
<span id="cb16-9"><a href="notes-of-optimization.html#cb16-9" aria-hidden="true"></a><span class="kw">quad.naive</span>(z, S) <span class="op">%&gt;%</span><span class="st"> </span>summary</span>
<span id="cb16-10"><a href="notes-of-optimization.html#cb16-10" aria-hidden="true"></a><span class="co">#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. </span></span>
<span id="cb16-11"><a href="notes-of-optimization.html#cb16-11" aria-hidden="true"></a><span class="co">#&gt;   76.78   93.31   99.78  100.34  107.33  129.04</span></span></code></pre></div>
<p>Now, a version that use the Choleskey decomposition</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="notes-of-optimization.html#cb17-1" aria-hidden="true"></a>quad.chol &lt;-<span class="st"> </span><span class="cf">function</span>(z, S) {</span>
<span id="cb17-2"><a href="notes-of-optimization.html#cb17-2" aria-hidden="true"></a>        R &lt;-<span class="st"> </span><span class="kw">chol</span>(S)</span>
<span id="cb17-3"><a href="notes-of-optimization.html#cb17-3" aria-hidden="true"></a>        v &lt;-<span class="st"> </span><span class="kw">backsolve</span>(R, <span class="kw">t</span>(z), <span class="dt">transpose =</span> <span class="ot">TRUE</span>)</span>
<span id="cb17-4"><a href="notes-of-optimization.html#cb17-4" aria-hidden="true"></a>        <span class="kw">colSums</span>(v <span class="op">*</span><span class="st"> </span>v)</span>
<span id="cb17-5"><a href="notes-of-optimization.html#cb17-5" aria-hidden="true"></a>}</span>
<span id="cb17-6"><a href="notes-of-optimization.html#cb17-6" aria-hidden="true"></a><span class="kw">quad.chol</span>(z, S) <span class="op">%&gt;%</span><span class="st"> </span>summary</span>
<span id="cb17-7"><a href="notes-of-optimization.html#cb17-7" aria-hidden="true"></a><span class="co">#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. </span></span>
<span id="cb17-8"><a href="notes-of-optimization.html#cb17-8" aria-hidden="true"></a><span class="co">#&gt;   76.78   93.31   99.78  100.34  107.33  129.04</span></span></code></pre></div>
<p>Comparing both approaches</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="notes-of-optimization.html#cb18-1" aria-hidden="true"></a><span class="kw">library</span>(microbenchmark)</span>
<span id="cb18-2"><a href="notes-of-optimization.html#cb18-2" aria-hidden="true"></a><span class="kw">microbenchmark</span>(<span class="kw">quad.naive</span>(z, S), <span class="kw">quad.chol</span>(z, S))</span>
<span id="cb18-3"><a href="notes-of-optimization.html#cb18-3" aria-hidden="true"></a><span class="co">#&gt; Unit: milliseconds</span></span>
<span id="cb18-4"><a href="notes-of-optimization.html#cb18-4" aria-hidden="true"></a><span class="co">#&gt;              expr      min       lq     mean   median       uq      max neval</span></span>
<span id="cb18-5"><a href="notes-of-optimization.html#cb18-5" aria-hidden="true"></a><span class="co">#&gt;  quad.naive(z, S) 3.085939 3.251968 4.574137 3.461480 4.986992 20.40842   100</span></span>
<span id="cb18-6"><a href="notes-of-optimization.html#cb18-6" aria-hidden="true"></a><span class="co">#&gt;   quad.chol(z, S) 1.474169 1.574538 2.960061 2.004181 2.811306 31.29867   100</span></span>
<span id="cb18-7"><a href="notes-of-optimization.html#cb18-7" aria-hidden="true"></a><span class="co">#&gt;  cld</span></span>
<span id="cb18-8"><a href="notes-of-optimization.html#cb18-8" aria-hidden="true"></a><span class="co">#&gt;    b</span></span>
<span id="cb18-9"><a href="notes-of-optimization.html#cb18-9" aria-hidden="true"></a><span class="co">#&gt;   a</span></span></code></pre></div>
<p>The Choelsky decomposition is faster. Also, because <strong>we know</strong> that the covariance matrix in a multivariate Normal is symmetric and positive define we can use the Choleskey decomposition. The naive version does not have that information, therefore, inverte the matrix and takes more time to estimate!</p>

<!-- sna_r_python.md is generated from sna_r_python.Rmd. Please edit that file -->
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="notes-of-r-and-python.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
